\documentclass[11pt]{article}
\usepackage{amsmath,amsbsy,amssymb,verbatim,fullpage,ifthen,graphicx,bm,amsfonts,amsthm,url}
\usepackage{graphicx}
\usepackage{xcolor}
\newcommand{\mfile}[1]  {{\small \verbatiminput{./#1}}} % Jeff Fessler, input matlab file
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%\newcommand*{\qed}{\hfill\ensuremath{\blacksquare}}%
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\minimize}{\operatorname*{minimize\ }}
\newcommand{\maximize}{\operatorname*{maximize}}
\newcommand{\opdet}[1]{\operatorname{\textbf{det}}\left(#1\right)}
\newcommand{\optr}[1]{\operatorname{\textbf{tr}}\left(#1\right)}
\newcommand{\AnswerDefine}{}
\newcommand{\answer}[2][blue]{\ifdefined\AnswerDefine{\color{#1}\it#2}\fi}
\newcommand{\mtx}[1]{\mathbf{#1}}
\newcommand{\vct}[1]{\mathbf{#1}}
\def \lg       {\langle}
\def \rg       {\rangle}
\def \mA {\mtx{A}}
\def \mF {\mtx{F}}
\def \mG {\mtx{G}}
\def \mI {\mtx{I}}
\def \mJ {\mtx{J}}
\def \mU {\mtx{U}}
\def \mS {\mtx{S}}
\def \mV {\mtx{V}}
\def \mW {\mtx{W}}
\def \mLambda {\mtx{\Lambda}}
\def \mSigma {\mtx{\Sigma}}
\def \mX {\mtx{X}}
\def \mY {\mtx{Y}}
\def \mZ {\mtx{Z}}
\def \zero     {\mathbf{0}}
\def \vzero    {\vct{0}}
\def \vone    {\vct{1}}
\def \va {\vct{a}}
\def \vg {\vct{g}}
\def \vu {\vct{u}}
\def \vv {\vct{v}}
\def \vx {\vct{x}}
\def \vy {\vct{y}}
\def \vz {\vct{z}}
\def \vphi {\vct{\phi}}
\def \vmu {\vct{\mu}}
\def \R {\mathbb{R}}

%\newcommand{\st}{\operatorname*{\ subject\ to\ }}
\usepackage{algorithm,algpseudocode}
\usepackage{xspace}
% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot} \def\st{\emph{s.t}\onedot}
\pagestyle{plain}


\title{{\bf Homework Set 4, CPSC 8420, Spring 2022}} % Change to the appropriate homework number
\author{\Large\underline{Last Name, First Name}}
\date{\textbf{\Large\textcolor{red}{Due 04/17/2022, Sunday, 11:59PM EST}}} % put your name in the LastName, FirstName format
%\date{\today}

\begin{document}
\maketitle

\section*{Problem 1}
Considering soft margin SVM, where we have the objective and constraints as follows:
\begin{equation}\label{eq:1}
	\begin{aligned}
		min\;\; &\frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i\\s.t.  \;\; y_i(w^Tx_i + &b)  \geq 1 - \xi_i \;\;(i =1,2,...m)\\\xi_i \geq &0 \;\;(i =1,2,...m)
	\end{aligned}
\end{equation}
Now we formulate another formulation as:
\begin{equation}
	\begin{aligned}
		min\;\; &\frac{1}{2}||w||_2^2 +\frac{C}{2}\sum\limits_{i=1}^{m}\xi_i^2\\s.t.  \;\; y_i(w^Tx_i + &b)  \geq 1 - \xi_i \;\;(i =1,2,...m)
	\end{aligned}
\end{equation}
\begin{enumerate}
	\item Different from Eq. (\ref{eq:1}), we now drop the non-negative constraint for $\xi_i$, please show that optimal value of the objective will be the same when $\xi_i$ constraint is removed.\\\\
	The optimal value will still be the same since the objective is still convex and therefore be differentiated. Question 1.3 shows that the Lagrangian of this formulation can still be minimized and 1.4 shows that the dual version is the same as the dual for equation 1. 
	\item What's the generalized Lagrangian of the new soft margin SVM optimization problem?
	\begin{equation}
	L(w, b, \xi, \alpha) = \frac{1}{2}||w||^2 + \frac{C}{2}\sum^m_{i=1}\xi_i^2 - \sum^m_{i=1}\alpha_i[y_i(w^T x_i, b) - 1 + \xi_i]
	\end{equation}
	\item Now please minimize the Lagrangian with respect to $w, b$, and $\xi$.\\\\
	To minimize the Lagrangian we take the derivative with respect to $w, b, \xi$ and set to 0.
	\begin{equation}
	\frac{\partial L}{\partial w} = w - \sum^m_{i=1}\alpha_i y_i x_i = 0
	\end{equation}
	\begin{equation}
	\frac{\partial L}{\partial b} = \sum^m_{i=1}\alpha_i y_i = 0
	\end{equation}
	\begin{equation}
	\frac{\partial L}{\partial \xi} = C \xi_i - \alpha_i = 0
	\end{equation}
	\item What is the dual of this version soft margin SVM optimization problem? (should be similar to Eq. (10) in the slides)\\\\
	To derive the dual we apply the variables above to the Lagrangian. \\
	\begin{center}
	$\underbrace{min}_{w, b, \xi} L(w, b, \xi, \alpha) = \frac{1}{2}||w||^2 + \frac{C}{2}\displaystyle\sum^m_{i=1}\xi_i^2 - \displaystyle\sum^m_{i=1}\alpha_i[y_i(w^T x_i, b) - 1 + \xi_i] $\\
	$\underbrace{min}_{w, b, \xi} L(w, b, \xi, \alpha) = \frac{1}{2}||w||^2 - \displaystyle\sum^m_{i=1}\alpha_i[y_i(w^T x_i, b) - 1]$\\
	=$\frac{1}{2}||\sum^m_{i=1}\alpha_i y_i x_i||^2 - \displaystyle\sum^m_{i=1}\alpha_i[y_i(w^T x_i, b) - 1]$\\
	=$\frac{1}{2}w^T \displaystyle\sum^m_{i=1}\alpha_i y_i x_i - w^T \displaystyle\sum^m_{i=1}\alpha_i y_i x_i - \displaystyle\sum^m_{i=1}\alpha_i y_i b + \displaystyle\sum^m_{i=1}\alpha_i$\\
	=$-\frac{1}{2}w^T \displaystyle\sum^m_{i=1}\alpha_i y_i x_i - \displaystyle\sum^m_{i=1}\alpha_i y_i b + \displaystyle\sum^m_{i=1}\alpha_i$\\
	=$-\frac{1}{2}(\displaystyle\sum^m_{i=1}\alpha_i y_i x_i^T) \displaystyle\sum^m_{i=1}\alpha_i y_i x_i - b\displaystyle\sum^m_{i=1}\alpha_i y_i + \displaystyle\sum^m_{i=1}\alpha_i$=\\
	=$-\frac{1}{2}\displaystyle\sum^m_{i=1 j=1}\alpha_i \alpha_j y_i y_j x_i^T x_j +\displaystyle\sum^m_{i=1}\alpha_i$\\
	$\underbrace{max}_{\alpha}-\frac{1}{2}\displaystyle\sum^m_{i=1 j=1}\alpha_i \alpha_j y_i y_j x_i^T x_j +\displaystyle\sum^m_{i=1}\alpha_i$ which is same as\\
	$\underbrace{min}_{\alpha}\displaystyle\sum^m_{i=1}\alpha_i -\frac{1}{2}\displaystyle\sum^m_{i=1 j=1}\alpha_i \alpha_j y_i y_j x_i^T x_j $\\
	$s.t. \displaystyle\sum^m_{i=1}\alpha_i y_i = 0$\\
	$\alpha_i \ge 0 i = 1,2...m$
	
	
	\end{center}
	\item Please analysis bias-variance trade-off when $C$ increases.\\\\ When $C$ is small, the SVM classifier does not allow a lot of misclassification.  The support vector classifier will have low bias but may not generalize well and have high variance. Similarly, If $C$ is large, the number of misclassifications allowed has been increased. This classifier would generalize better but will have a higher amount of bias with lower variance.
\end{enumerate}

\section*{Problem 2}
Recall vanilla SVM objective:
\begin{equation}
\begin{aligned}
L(w,b,\alpha) = \frac{1}{2}||w||_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \; \quad s.t. \quad \alpha_i \geq 0
\end{aligned}
\end{equation}
If we denote the margin as $\gamma$, and vector $\alpha=[\alpha_1, \alpha_2, \dots, \alpha_m]$, now please show $\gamma^2*\|\alpha\|_1=1$.\\

Since support vectors lie on hyperplanes, for any support vector $x_i, w * x_i + b = y_i$ and therefore $b$ can be obtained by letting $b = y_i - \displaystyle\sum_{j=1}^m \alpha_i y_i (x_j \cdot x_i)$\\
If we multiply both sides by $\alpha_i y_i$ then we get:\\
$\displaystyle\sum_{j=1}^m \alpha_i y_i b = \displaystyle\sum_{j=1}^m \alpha_i y_i^2
 - \displaystyle\sum_{j=1}^m \alpha_i \alpha_j y_iy_j (x_i \cdot x_j)$\\
 We use the fact that $y_i = 1$ and $\gamma = \frac{1}{||w||}$ along with $w = \displaystyle\sum_{i=1}^m \alpha_i y_i x_i$\\
 Therefore $0 = \displaystyle\sum_{i=1}^m \alpha_i - ||w||^2$\\\\
 $\gamma^2 = \frac{1}{||w||^2} = \frac{1}{\displaystyle\sum_{i=1}^m \alpha_i} = \frac{1}{||\alpha||}$ so $\gamma^2 * ||\alpha|| = 1$
  \end{document}

\documentclass[11pt]{article}
\usepackage{amsmath,amsbsy,amssymb,verbatim,fullpage,ifthen,graphicx,bm,amsfonts,amsthm,url}
\usepackage{graphicx}
\usepackage{xcolor}
\newcommand{\mfile}[1]  {{\small \verbatiminput{./#1}}} % Jeff Fessler, input matlab file
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%\newcommand*{\qed}{\hfill\ensuremath{\blacksquare}}%
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\minimize}{\operatorname*{minimize\ }}
\newcommand{\maximize}{\operatorname*{maximize}}
\newcommand{\opdet}[1]{\operatorname{\textbf{det}}\left(#1\right)}
\newcommand{\optr}[1]{\operatorname{\textbf{tr}}\left(#1\right)}
%\newcommand{\AnswerDefine}{}
\newcommand{\answer}[2][blue]{\ifdefined\AnswerDefine{\color{#1}\it#2}\fi}
\newcommand{\mtx}[1]{\mathbf{#1}}
\newcommand{\vct}[1]{\mathbf{#1}}
\def \lg       {\langle}
\def \rg       {\rangle}
\def \mA {\mtx{A}}
\def \mF {\mtx{F}}
\def \mG {\mtx{G}}
\def \mI {\mtx{I}}
\def \mJ {\mtx{J}}
\def \mU {\mtx{U}}
\def \mS {\mtx{S}}
\def \mV {\mtx{V}}
\def \mW {\mtx{W}}
\def \mLambda {\mtx{\Lambda}}
\def \mSigma {\mtx{\Sigma}}
\def \mX {\mtx{X}}
\def \mY {\mtx{Y}}
\def \mZ {\mtx{Z}}
\def \zero     {\mathbf{0}}
\def \vzero    {\vct{0}}
\def \vone    {\vct{1}}
\def \va {\vct{a}}
\def \vg {\vct{g}}
\def \vu {\vct{u}}
\def \vv {\vct{v}}
\def \vw {\vct{w}}
\def \vx {\vct{x}}
\def \vy {\vct{y}}
\def \vz {\vct{z}}
\def \vphi {\vct{\phi}}
\def \vmu {\vct{\mu}}
\def \R {\mathbb{R}}

%\newcommand{\st}{\operatorname*{\ subject\ to\ }}
\usepackage{algorithm,algpseudocode}
\usepackage{xspace}
% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot} \def\st{\emph{s.t}\onedot}
\pagestyle{plain}

\title{{\bf Homework Set 5, CPSC 8420, Spring 2022}} % Change to the appropriate homework number
\author{\Large\underline{Sherrer, Will}}
\date{\textbf{\Large\textcolor{red}{Due 04/28/2022, Thursday, 11:59PM EST}}} % put your name in the LastName, FirstName format
%\date{\today}

\begin{document}
\maketitle
\section*{Problem 1}
Recall the classification models we discussed in class: \textbf{LDA}, \textbf{SVM} and \textbf{Logistic Regression}, seems all of them work on binary classification task. However, in real-world applications, multi-classification is everywhere, thus in this problem we explore how to extend vanilla \textbf{Logistic Regression} for multi-classification. Assume we have $K$ different classes and the input $
\vx\in\mathcal{R}^d$, and the probability to each class is defined as:
\begin{equation}
	%\begin{aligned}
		P(Y=k|X=\vx) =  \frac{exp(\vw_k^T\vx)}{ 1+\sum_{l=1}^{K-1}exp(\vw_l^T\vx)} \quad for  \quad k=1,2,\dots,K-1; P(Y=K|X=\vx) =  \frac{1} { 1+\sum_{l=1}^{K-1}exp(\vw_l^T\vx)}
	%\end{aligned}
\end{equation}
If we define $\vw_K=\vzero$, then we can combine the two cases above as one:
\begin{equation}
	P(Y=k|X=\vx) =  \frac{exp(\vw_k^T\vx)}{ 1+\sum_{l=1}^{K-1}exp(\vw_l^T\vx)} \quad for  \quad k=1,\dots,K
\end{equation}
\begin{enumerate}
	\item What and how many parameters are there to be optimized? \\
	The parameters are $\{w_1, w_2...., w_{K-1}\}$ where they are a d-dimensional vector. There are a total of $(K - 1) * d$ parameters

	\item The training data is given as: $\{(\vx_1,y_1),\dots,(\vx_n,y_n)\}$, please simplify the log likelihood function to your best:
	\begin{equation}\label{ori}
		L(\vw_1,\dots,\vw_{K-1})=\sum\limits_{i=1}^n ln P(Y=y_i|X=\vx_i)
	\end{equation}
	
	$L(\vw_1,\dots,\vw_{K-1})=\displaystyle\sum\limits_{i=1}^n ln P(Y=y_i|X=\vx_i)$\\
	$=\displaystyle\sum^n_{i=1} ln \frac{exp(\vw_k^T\vx)}{ 1+\displaystyle\sum\limits_{l=1}^{K-1}exp(\vw_l^T\vx)}$\\	
	$=\displaystyle\sum^n_{i=1} [w^T_{yi} x - ln(1 + \displaystyle\sum^{K - 1}_{l=1} exp(w^T_l x))]$
	\item Now please find the gradient of $L$ \wrt $\vw_k$.\\
	$\nabla L(w_k) =\displaystyle\sum^n_{i=1} [I(y_i = k) x_i - \frac{exp(\vw_k^T\vx)}{ 1+\displaystyle\sum\limits_{l=1}^{K-1}exp(\vw_l^T\vx)} x_i]$\\
	$=\displaystyle\sum^m_{i=1} [I(y_i = k) - P(y = k|X = x_i)]x_i$
	\item If we add regularization term and formulate new objective function as:
	\begin{equation}\label{reg}
		f(\vw_1,\dots,\vw_{K-1})=L(\vw_1,\dots,\vw_{K-1})-\frac{\lambda}{2}\sum\limits_{l=1}^{K-1}\|\vw_l\|^2_2,
	\end{equation}
	now please determine the new gradient.\\
	$\nabla f(w_k) = \nabla (w_k) - \lambda (w_k)$
	\item You are given \textit{USPS} handwritten recognition digit dataset, with image size $16 \times 16$. For each digit (\ie 0,1,\dots,9) there are 600 training samples in addition to 500 testing ones. You may use: imshow(reshape($\vx$,16,16)) to view the image in Matlab. (Non-Matlab user may utilize .txt files to conduct experiments.)
	\begin{enumerate}
		\item Please use gradient ascent algorithm (you are expected to complete log\_grad.m) to train the model and plot 1) vanilla objective function L in Eq.(\ref{ori}); 2) training accuracy and 3) testing accuracy with updates respectively. Also indicate the final testing accuracy score. (Please choose a proper learning rate and stopping criterion). The folder include figures for your reference. 
		\includegraphics[scale=.5]{obj.pdf}
		
		\includegraphics[scale=.6]{train_err.pdf}\\
		\includegraphics[scale=.6]{test_err.pdf}
		\item Now if we add the regularization term as Eq.(\ref{reg}), please show the final accuracy when $\lambda=\{0,1,10,100,200\}$ respectively.\\
		\begin{tabular}{|c|c|c|c|c|c| }
		\hline
		$\lambda$ & 0 & 1  & 10 & 100 & 200 \\
		Acc. & 0.9144 &  0.9158 &  0.9192 &  0.8974  & 0.8818\\
		\hline
		
		\end{tabular}
		\item What conclusion can we draw from the above experiments?\\
		To conclude the experiments show that it is possible for the regularization to avoid overfitting the data, namely when $\lambda = 10$, however the regularization can't be too large as to not disturb completely. 
	\end{enumerate}
\end{enumerate}

\textbf{log Grad Code}\\
\lstinputlisting{log_grad.m}

\end{document}
